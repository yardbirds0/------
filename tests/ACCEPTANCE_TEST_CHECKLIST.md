# Model Dialogs 验收测试检查清单

## 测试准备

### 前置条件
- [ ] 项目依赖已安装 (PySide6等)
- [ ] config/ai_models.json 文件存在
- [ ] 至少有一个provider配置

### 测试环境
- 操作系统: Windows
- Python版本: 3.13
- PySide6版本: 6.x
- 测试日期: 2025-10-06

---

## 第一部分: 代码级验证 (自动化测试)

### T1.1 导入验证
- [x] AddModelDialog 可以成功导入
- [x] ModelBrowserDialog 可以成功导入
- [x] ModelConfigDialog 包含新增的4个方法
- [x] 所有类的Signal定义正确

**测试方法**:
```bash
python tests/verify_model_dialogs_import.py
```

**预期结果**: 所有检查显示 [OK]

**实际结果**: ✅ PASSED - 所有导入和方法检查通过

---

## 第二部分: AddModelDialog (添加模型对话框)

### T2.1 UI布局验证
运行: `python tests/acceptance_test_model_dialogs.py`
点击 "打开模型配置窗口" → "添加模型"

- [ ] 对话框标题显示 "添加模型"
- [ ] 对话框尺寸为 480×360 像素
- [ ] 对话框为模态窗口(无法操作后面的窗口)
- [ ] 标题文字显示 "添加自定义模型"

### T2.2 输入字段验证
- [ ] 显示3个输入字段
- [ ] 第1个字段: "模型 ID" 标签，带红色 * 标记 (必填)
- [ ] 第1个字段: 有帮助图标 (?) 显示提示文本
- [ ] 第2个字段: "模型名称" 标签，无 * 标记 (可选)
- [ ] 第3个字段: "分组名称" 标签，无 * 标记 (可选)
- [ ] 所有输入框高度一致，圆角边框

### T2.3 按钮验证
- [ ] 显示 "取消" 按钮 (灰色)
- [ ] 显示 "添加模型" 按钮 (绿色)
- [ ] 按钮位于对话框底部右侧
- [ ] 鼠标悬停时按钮颜色变化

### T2.4 功能测试 - 空输入验证
**测试步骤**:
1. 不输入任何内容
2. 点击 "添加模型" 按钮

**预期结果**:
- [ ] 显示错误消息 "模型ID不能为空"
- [ ] 焦点返回到模型ID输入框
- [ ] 对话框不关闭

### T2.5 功能测试 - 格式验证
**测试步骤**:
1. 在模型ID输入: `invalid@model#name`
2. 点击 "添加模型" 按钮

**预期结果**:
- [ ] 显示错误消息 "模型ID格式无效..."
- [ ] 对话框不关闭

### T2.6 功能测试 - 唯一性验证
**测试步骤**:
1. 在模型ID输入已存在的模型ID (如: "Qwen/Qwen2.5-7B-Instruct")
2. 点击 "添加模型" 按钮

**预期结果**:
- [ ] 显示错误消息 "模型ID 'xxx' 已存在"
- [ ] 对话框不关闭

### T2.7 功能测试 - 成功添加
**测试步骤**:
1. 模型ID输入: `test-model-acceptance-001`
2. 模型名称输入: `测试模型001`
3. 分组名称输入: `验收测试`
4. 点击 "添加模型" 按钮

**预期结果**:
- [ ] 显示成功消息 "模型 '测试模型001' 已成功添加到 xxx"
- [ ] 对话框自动关闭
- [ ] 模型配置窗口的模型列表刷新
- [ ] 新模型出现在列表中

### T2.8 功能测试 - 取消操作
**测试步骤**:
1. 输入任意内容
2. 点击 "取消" 按钮

**预期结果**:
- [ ] 对话框关闭
- [ ] 不保存任何数据

### T2.9 Cherry Theme样式验证
- [ ] 背景色为白色 (#FFFFFF)
- [ ] 输入框背景为浅灰 (#F7F8FA)
- [ ] 边框颜色为 #E5E7EB
- [ ] 聚焦时边框变为蓝色 (#3B82F6)
- [ ] 添加按钮背景为绿色 (#10B981)
- [ ] 字体为微软雅黑
- [ ] 圆角为8px

---

## 第三部分: ModelBrowserDialog (管理模型对话框)

### T3.1 UI布局验证
运行: `python tests/acceptance_test_model_dialogs.py`
点击 "打开模型配置窗口" → "管理模型"

- [ ] 对话框标题显示 "硅基流动模型"
- [ ] 对话框尺寸为 1000×680 像素
- [ ] 对话框为模态窗口

### T3.2 搜索栏验证
- [ ] 显示搜索图标 (🔍)
- [ ] 搜索输入框占据大部分宽度
- [ ] 占位符文字: "搜索模型 ID 或名称"
- [ ] 显示筛选按钮 (⚙)
- [ ] 显示刷新按钮 (🔄)

### T3.3 分类标签验证
- [ ] 显示8个分类按钮
- [ ] 分类: 全部、推理、视觉、联网、免费、嵌入、重排、工具
- [ ] "全部" 标签默认为绿色激活状态
- [ ] 其他标签为灰色未激活状态

### T3.4 模型树验证
- [ ] 显示QTreeWidget树形控件
- [ ] Provider按分组显示
- [ ] 每个Provider显示模型数量 (如: "硅基流动  14")
- [ ] Provider默认展开状态
- [ ] 模型显示名称和状态标签

### T3.5 模型标签验证
- [ ] 内置模型显示 [内置] 标签
- [ ] 自定义模型显示 [自定义] 标签
- [ ] 标签显示在模型名称后面

### T3.6 功能测试 - 搜索
**测试步骤**:
1. 在搜索框输入 "Qwen"
2. 等待300ms

**预期结果**:
- [ ] 只显示包含 "Qwen" 的模型
- [ ] 不包含该关键词的Provider被隐藏
- [ ] 搜索不区分大小写

### T3.7 功能测试 - 分类筛选
**测试步骤**:
1. 点击 "嵌入" 分类标签

**预期结果**:
- [ ] "嵌入" 标签变为绿色激活
- [ ] "全部" 标签变为灰色未激活
- [ ] 只显示Embedding类别的模型
- [ ] Provider数量显示更新

### T3.8 功能测试 - 组合筛选
**测试步骤**:
1. 点击 "推理" 分类
2. 在搜索框输入 "DeepSeek"

**预期结果**:
- [ ] 同时应用分类和搜索过滤
- [ ] 只显示推理类别中包含DeepSeek的模型

### T3.9 功能测试 - 模型选择
**测试步骤**:
1. 点击任意模型项

**预期结果**:
- [ ] 对话框自动关闭
- [ ] 该模型被设置为当前激活模型
- [ ] 主窗口的当前模型显示更新

### T3.10 功能测试 - 刷新
**测试步骤**:
1. 点击刷新按钮 (🔄)

**预期结果**:
- [ ] 重新加载配置文件
- [ ] 树形列表刷新
- [ ] 新添加的模型出现

### T3.11 Cherry Theme样式验证
- [ ] 背景色为白色
- [ ] 搜索框样式与cherry_theme一致
- [ ] 激活的分类标签为绿色 (#10B981)
- [ ] 树形控件边框为 #E5E7EB
- [ ] 鼠标悬停高亮为浅灰 (#F0F1F3)

---

## 第四部分: ModelConfigDialog集成验证

### T4.1 按钮连接验证
运行: `python tests/acceptance_test_model_dialogs.py`

- [ ] "添加模型" 按钮存在
- [ ] "管理模型" 按钮存在
- [ ] 点击 "添加模型" 打开AddModelDialog
- [ ] 点击 "管理模型" 打开ModelBrowserDialog
- [ ] 没有显示占位符消息

### T4.2 UI刷新验证
**测试步骤**:
1. 打开ModelConfigDialog
2. 点击 "添加模型"，添加新模型
3. 关闭AddModelDialog

**预期结果**:
- [ ] 模型配置窗口的Provider列表刷新
- [ ] 新模型出现在模型树中
- [ ] 新模型被自动选中

### T4.3 模型切换验证
**测试步骤**:
1. 打开ModelConfigDialog
2. 点击 "管理模型"
3. 选择不同的模型

**预期结果**:
- [ ] 模型配置窗口的当前选择更新
- [ ] 右侧模型详情显示新模型信息

---

## 第五部分: 数据持久化验证

### T5.1 JSON文件更新
**测试步骤**:
1. 添加新模型 `test-persistence-001`
2. 关闭所有窗口
3. 检查 `config/ai_models.json` 文件

**预期结果**:
- [ ] JSON文件包含新模型
- [ ] 模型数据结构正确:
  - [ ] id: "test-persistence-001"
  - [ ] name: (用户输入的名称)
  - [ ] category: (用户输入的分组)
  - [ ] custom: true
  - [ ] context_length: 8192
  - [ ] max_tokens: 4096

### T5.2 配置持久化
**测试步骤**:
1. 重新运行程序
2. 打开模型配置窗口

**预期结果**:
- [ ] 之前添加的自定义模型仍然存在
- [ ] 显示 [自定义] 标签

---

## 第六部分: 错误处理验证

### T6.1 无Provider时添加模型
**测试步骤**:
1. 修改config使current_provider为空
2. 点击 "添加模型"

**预期结果**:
- [ ] 显示警告 "请先选择一个服务商"
- [ ] 对话框不打开

### T6.2 配置文件损坏
**测试步骤**:
1. 临时损坏ai_models.json
2. 启动程序

**预期结果**:
- [ ] 加载默认配置
- [ ] 显示错误信息
- [ ] 程序不崩溃

---

## 验收标准总结

### 必须通过的测试 (Critical)
- [x] T1.1 - 代码导入验证
- [ ] T2.1-T2.3 - AddModelDialog UI
- [ ] T2.7 - 成功添加模型
- [ ] T3.1-T3.5 - ModelBrowserDialog UI
- [ ] T3.9 - 模型选择
- [ ] T4.1 - 按钮集成
- [ ] T5.1 - 数据持久化

### 应该通过的测试 (Important)
- [ ] T2.4-T2.6 - 输入验证
- [ ] T3.6-T3.8 - 搜索和筛选
- [ ] T4.2-T4.3 - UI刷新
- [ ] T6.1-T6.2 - 错误处理

### 可选验证 (Nice to Have)
- [ ] T2.9 - Cherry Theme样式细节
- [ ] T3.11 - Cherry Theme样式细节
- [ ] T5.2 - 长期持久化

---

## 测试执行记录

### 自动化测试
- **执行时间**: 2025-10-06
- **执行命令**: `python tests/verify_model_dialogs_import.py`
- **结果**: ✅ PASSED
- **详情**: 所有导入和方法检查通过

### 手动GUI测试
- **执行时间**: _待执行_
- **执行者**: _待分配_
- **测试脚本**: `python tests/acceptance_test_model_dialogs.py`
- **结果**: _待填写_

---

## 验收决策

- [ ] 所有Critical测试通过
- [ ] 至少80%的Important测试通过
- [ ] 无阻塞性bug
- [ ] UI符合设计要求
- [ ] 数据持久化正常

**最终决策**: _待验收_

**签字**: __________
**日期**: __________
